# -*- coding: utf-8 -*-
"""AnaliseSentimentoNoticia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qf_w9VvuJJMNZ9rqj_j7IIbEjK-KOmAD
"""

##################################################################################################
#Instalando Pacotes
##################################################################################################
#pip install pandas
#pip install Unidecode
#pip install requests
#pip install json
#pip install typing
#pip install matplotlib
#pip install numpy
#pip install nltk
#pip install sklearn
#pip install wordcloud
##################################################################################################

# Commented out IPython magic to ensure Python compatibility.
##################################################################################################
#Importando pacotes a serem utilizados no projeto
##################################################################################################
import pandas as pd
import requests
import json
from pandas.io.json import json_normalize
from typing import List
from datetime import date, datetime
import matplotlib.pyplot as plt
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from wordcloud import WordCloud
import unidecode
from unidecode import unidecode
import unicodedata
import re
import io
nltk.download('stopwords')
nltk.download('rslp')
nltk.download('punkt')
# %matplotlib inline
lista_stop =  nltk.corpus.stopwords.words('portuguese')
##################################################################################################

##################################################################################################
#RemoveStopWords
#Stopwords são palavras comuns que normalmente não contribuem para o significado de uma frase, 
#pelo menos com relação ao propósito da informação e do processamento da linguagem natural
##################################################################################################

np.transpose(lista_stop)

#Adicionando novas StopWords
lista_stop.append('tao')
lista_stop.append('tudo')
lista_stop.append('ser')
lista_stop.append('sobre')	
lista_stop.append('do')
lista_stop.append('quais')
lista_stop.append('sao')
lista_stop.append('ter')
lista_stop.append('ate')
##################################################################################################

##################################################################################################
#Remoção de acentos.
##################################################################################################
def removerAcentosECaracteresEspeciais(palavra):
      # Unicode normalize transforma um caracter em seu equivalente em latin.
      nfkd = unicodedata.normalize('NFKD', palavra.replace("'",""))
      palavraSemAcento = u"".join([c for c in nfkd if not unicodedata.combining(c)])
      # Usa expressão regular para retornar a palavra apenas com números, letras e espaço
      return re.sub('[^a-zA-Z0-9 \\\]', '', palavraSemAcento)
##################################################################################################

##################################################################################################
#Remoção de acentos e StopWords
##################################################################################################
def preprocess_Dataset_Treinamento(text):
   stemmer = nltk.stem.RSLPStemmer()
   frases_sem_Stemming = []

   for (palavras, sentimento, classificacao) in text:
     #com_Stemming = [str(stemmer.stem(p)) for p in removerAcentosECaracteresEspeciais(palavras).lower().split()  if p not in lista_stop]
     com_Stemming = [str(p) for p in removerAcentosECaracteresEspeciais(palavras).lower().split()  if p not in lista_stop]
     frases_sem_Stemming.append((com_Stemming,sentimento,classificacao))
   return frases_sem_Stemming
##################################################################################################

##################################################################################################
#Remoção de acentos e StopWords
##################################################################################################
def preprocess_Dataset_Noticias(text):
   stemmer = nltk.stem.RSLPStemmer()
   frases_sem_Stemming = []

   for (palavras) in text:
     #com_Stemming = [str(stemmer.stem(p)) for p in removerAcentosECaracteresEspeciais(palavras).lower().split()  if p not in lista_stop]
     com_Stemming = [str(p) for p in removerAcentosECaracteresEspeciais(palavras).lower().split() if p not in lista_stop]
     frases_sem_Stemming.append(' '.join(map(str, com_Stemming)))
   return frases_sem_Stemming
##################################################################################################

##################################################################################################
#Base de Treinamento Sentimento Noticias
##################################################################################################
#Dataset de treinamento retiraco dos seguintes sites:
#https://portaldobitcoin.com/
#https://cointimes.com.br/
#https://guiadobitcoin.com.br/
##################################################################################################
df = pd.read_csv('Base_Treinamento_Noticias.csv',encoding='utf-8', delimiter=';')
df.head().T
dados = [tuple(map(lambda x: str(x).replace('\xa0',' ') if type(x) == str else x, tup)) 
        for tup in tuple(tuple(x) for x in df.values.tolist())]
print(dados)
##################################################################################################


##################################################################################################
#Remoção de acentos e remove StopWords
##################################################################################################
retorno_base = preprocess_Dataset_Treinamento(dados)
##################################################################################################

##################################################################################################
#Criação do DataFrame
##################################################################################################
df_frases_com_Stem_treinamento = pd.DataFrame(retorno_base)
df_frases_com_Stem_treinamento.columns = ['Frase', 'Sentimento','Classificacao']
##################################################################################################

##################################################################################################
#Gráfico Barras
##################################################################################################
#Podemos observar que não temos o problema de desbalanceamento das classes.
print('Tamanho da base de treinamento (Frases): ', format(df_frases_com_Stem_treinamento.shape[0]))
df_frases_com_Stem_treinamento.Sentimento.value_counts().plot(kind='bar')
##################################################################################################

##################################################################################################
#Dataframe informações gerais dos dados
##################################################################################################
print((df_frases_com_Stem_treinamento.Sentimento.value_counts() / df_frases_com_Stem_treinamento.shape[0]) * 100)
df_frases_com_Stem_treinamento.sample(n=1)
#display(df_frases_com_Stem_treinamento)
##################################################################################################

##################################################################################################
todas_Palavras = []
def busca_Palavras(frase):
	for (palavras, sentimento, classificacao) in frase:
		todas_Palavras.extend(palavras)
	return todas_Palavras
##################################################################################################

##################################################################################################
#Retorno do total da base de treinamento
##################################################################################################
retorno_busca_palavras = busca_Palavras(retorno_base)

palavras_treinamento = []

for palavra in retorno_busca_palavras:
	palavras_treinamento.append(palavra)

print("Quantidade de palavras na base de treinamento {}".format(pd.DataFrame(palavras_treinamento).count()))
##################################################################################################

##################################################################################################
#Quantidade de vezes que uma palavra aparece no Dataset de Treinamento
##################################################################################################
def busca_frequencia(palavras):
	palavras = nltk.FreqDist(palavras)
	return palavras
	
freq = busca_frequencia(palavras_treinamento)
pd.DataFrame(list(freq.items()), columns = ["Palavra","Frequencia"])
##################################################################################################

##################################################################################################
#Tokenização
#É o processo de tokenizar ou dividir uma string ou textos em uma lista de tokens. 
#O tokenizador  usa o espaço para separara as palavras.
##################################################################################################
def tokenização(lista) -> List:
    novalista: List = []
    for a in lista:
      for b in a[0]:
        palavras = nltk.FreqDist(b)
        novalista.append((b,str(a[1]),a[2]))   
    return novalista

teste = retorno_base

r = tokenização(teste)

df_frases_com_Stem_treinamento_Teste = pd.DataFrame(r)
df_frases_com_Stem_treinamento_Teste.columns = ['Frase', 'Sentimento','Classificacao']

#df_frases_com_Stem_treinamento_Teste.drop_duplicates(subset ="Frase", keep = False, inplace = True) 

#display(df_frases_com_Stem_treinamento_Teste)
##################################################################################################

##################################################################################################
#Função para retornar somente as palavras únicas
##################################################################################################
def busca_palavras_unicas(frequencia):
	freq = frequencia.keys()
	return freq

palavras_unicas_treinamento = busca_palavras_unicas(freq)
print(palavras_unicas_treinamento)
##################################################################################################

##################################################################################################
#IMPLEMENTAÇÃO NUVEM PALAVRAS JÁ TRADAS
##################################################################################################
words = []
for i in todas_Palavras:
    words.append(i)

words = (str(words))

wordcloud = WordCloud(min_font_size = 20, 
                      max_font_size = 300,
                      width = 2000,
                      height = 1000,
                      mode = "RGB").generate(words)
plt.figure(figsize = (16,9))
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")
plt.show()
##################################################################################################

##################################################################################################
#Convertemos Frase para uma matriz sparsa, e logo após imprimimos o vocabulário do CountVectorizer.
#com bigrama ngram_range (1,2) agrupamento de palavras
##################################################################################################
treinamento = pd.DataFrame(df_frases_com_Stem_treinamento_Teste['Frase'])
treinamento.columns = ['Palavra']

token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(lowercase=True,stop_words=None,ngram_range = (1,2),
                        tokenizer = token.tokenize)
text_counts = cv.fit_transform(treinamento['Palavra'])
#cv.vocabulary_
##################################################################################################

##################################################################################################
#Treina o modelo
#Agora vamos aplicar o tipo de aprendizado supervisionado, usando o Naive Bayes – MultinomiaNB, 
#aplicamos o treino e fazemos um predict dos dados de teste.
##################################################################################################
X_train, X_test, y_train, y_test = train_test_split(text_counts, 
                                                     df_frases_com_Stem_treinamento_Teste['Classificacao'],
                                                     test_size=0.34, random_state=1,shuffle=True)
clf = MultinomialNB().fit(X_train, y_train)
y_predicted = clf.predict(X_test)
##################################################################################################

##################################################################################################
#Matriz de confusão:
#Para ter uma visão mais clara dos erros e sentimentos que estão em conflito no nosso modelo, 
##################################################################################################
font = {'family' : 'normal',
	      'weight' : 'bold',
	      'size'	 :  13}
plt.rc('font',**font)

cm = metrics.confusion_matrix(y_test,y_predicted)

plt.figure(figsize=(14,9))
plt.clf()
plt.imshow(cm, interpolation='nearest',cmap=plt.cm.Wistia)
classNames = ['Negativo','Positivo','Neutro']
plt.title('Matriz de Confusão Base de Tetes Notícias')
plt.ylabel('Classe Verdadeira')
plt.xlabel('Previsão Classe')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks,classNames)
s = [['TN','FP','FNEU'],['FN','TP','FNEU'],['FN','FP','TNEU']]

for i in range(3):
  for j in range(3):
    plt.text(j,i,str(s[i][j])+"="+str(cm[i][j]))
plt.show()
##################################################################################################

##################################################################################################
#Acuracia do modelo
##################################################################################################
print("MultinomialNB Accuracy: ",metrics.accuracy_score(y_test,y_predicted).round(3))
##################################################################################################
#podemos considerar um resultado razoável 
#se levarmos em consideração o tamanho da amostra e a quantidade de labels (sentimentos), 
##################################################################################################

##################################################################################################
#Testando o Modelo Analise de Sentimento
##################################################################################################
palavras_treino = cv.fit_transform(treinamento['Palavra'])

classificacao_treino = df_frases_com_Stem_treinamento_Teste['Classificacao']
modelo = MultinomialNB()
modelo.fit(palavras_treino,classificacao_treino)

##################################################################################################

##################################################################################################
#Consultando API Json externo de notícias de Bitcoin para Análise de Sentimento
##################################################################################################

#Notícias Bitcoin Cointelegraph

url = 'https://api.rss2json.com/v1/api.json?rss_url=https%3A%2F%2Fcointelegraph.com.br%2Frss%2Ftag%2Fbitcoin'

result =  requests.get(url).json()
df = pd.DataFrame.from_dict([x[1] for x in result.items()][2])['title']
df.drop_duplicates() 
noticias = [x[1] for x in df.items()]
noticias_tratadas = preprocess_Dataset_Noticias(noticias)

##################################################################################################

#Notícias Bitcoin Bitcoinnews

url2 = 'https://api.rss2json.com/v1/api.json?rss_url=https://bitcoinnews.com.br/feed'

result2 =  requests.get(url2).json()
df2 = pd.DataFrame.from_dict([x[1] for x in result2.items()][2])['title']
df2.drop_duplicates() 
noticias2 = [x[1] for x in df2.items()]
noticias_tratadas2 = preprocess_Dataset_Noticias(noticias2)
print("Todas as Notícias Externas Tratadas: ")
todas_noticias_tratadas = noticias_tratadas + noticias_tratadas2
print(todas_noticias_tratadas)
freq_testes = cv.transform(todas_noticias_tratadas)

##################################################################################################

print (modelo.classes_)
modelo.predict_proba(freq_testes).round(2)

##################################################################################################
#Declaração das variáveis das classificações
##################################################################################################
possibilidade_noticias_dia_positiva = 0
possibilidade_noticias_dia_negativa = 0
possibilidade_noticias_dia_neutra = 0
##################################################################################################


##################################################################################################
#Peso estipulado para cada tipo de classificação, para tirar a média geral das noticias analisadas
#no dia, e estipular o sentimento do mercado.
#Foi estipulado o peso 3 para as noticias negativas
##################################################################################################
peso_noticia_negativa = 3
peso_noticia_positiva = 2
peso_noticia_neutra = 1
##################################################################################################


##################################################################################################
# Fazendo a classificação com o modelo treinado:
##################################################################################################
list_classificacao = []
for t, c in zip (noticias_tratadas,modelo.predict(freq_testes)):
    # t representa o noticia e c a classificação de cada noticia.
    #print (t +", "+ str(c))
    s = ''

    if c == 0:
      possibilidade_noticias_dia_negativa = possibilidade_noticias_dia_negativa + (1 * peso_noticia_negativa)
      s = 'negativa'
    if c == 1:
      possibilidade_noticias_dia_positiva = possibilidade_noticias_dia_positiva + (1 * peso_noticia_positiva)
      s = 'positiva'
    if c == 2:
      possibilidade_noticias_dia_neutra = possibilidade_noticias_dia_neutra + (1 * peso_noticia_neutra)
      s = 'neutra'

    list_classificacao.append([t,c,s])
##################################################################################################


##################################################################################################
#Calculando Média e Percentuais das Notícias Analisadas
##################################################################################################
total_possibilidade = possibilidade_noticias_dia_negativa + possibilidade_noticias_dia_positiva + possibilidade_noticias_dia_neutra

possibilidade_noticias_dia_negativa = (possibilidade_noticias_dia_negativa * 100) / total_possibilidade
possibilidade_noticias_dia_positiva = (possibilidade_noticias_dia_positiva * 100) / total_possibilidade
possibilidade_noticias_dia_neutra =   (possibilidade_noticias_dia_neutra * 100) / total_possibilidade

df_analise = pd.DataFrame(list_classificacao)
df_analise.columns = ['Frase', 'Classificacao','Sentimento']
#display(df_analise)

df_result = pd.DataFrame()
df_result['positiva'] = [round(possibilidade_noticias_dia_positiva,2)]
df_result['negativa'] = [round(possibilidade_noticias_dia_negativa,2)]
df_result['neutra'] =   [round(possibilidade_noticias_dia_neutra,2)]
##################################################################################################

##################################################################################################
#Criando Gráfico de Análise de Sentimento
##################################################################################################
data_atual = date.today().strftime('%d/%m/%Y')

def generate_piechart(df_result):
  import matplotlib.pyplot as plt
  labels = df_result.columns.tolist()
  sizes = df_result.values.tolist()[0]
  color = ['lightskyblue','lightcoral']
  explode = (0.15, 0, 0.15)
  fig1, ax1 = plt.subplots(figsize=(5,5))
  ax1.pie(sizes, labels=labels, explode=explode,
          shadow=True, autopct='%1.1f%%', startangle=140, colors=color)
  ax1.set_title('Gráfico Resultado Análise Sentimento Noticias do Dia ' + str(data_atual), fontsize=15)
  ax1.axis('equal')
  plt.show()
generate_piechart(df_result)
##################################################################################################